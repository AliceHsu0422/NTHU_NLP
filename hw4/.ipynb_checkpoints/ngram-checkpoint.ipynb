{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: N-gram Frequency\n",
    "\n",
    "During the natural language processing, sometimes we would like to know how many times a word, a phrase or a pattern has appeared in the text. This is called the **word/phrase frequency** .  \n",
    "For example, in the sentence *\\\"The more you read, the better your vocabulary becomes\\\"*, the fequency of *read* is `1`, while the frequency of *the* is `2` .  \n",
    "\n",
    "Word frequency can be used to gain some knowledge from the article.  \n",
    "As an example, if the frequency of the word *language* is much higher than other words in an article, we can assume that this article may be related to linguistic topics.  \n",
    "\n",
    "In this assignment, we will tell you what N-gram is and how to calculate N-gram frequency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Before we start counting words, of course we need to transform the sentence into *words* .  \n",
    "As what we want, **Tokenization** is the process to split a sentence into smaller word units, which we call *tokens* .  \n",
    "\n",
    "Take the sentence *\"Of course we're celebrating the New Year!\"* for example.  \n",
    "We need to break it into a token list like `[\"Of\", \"course\", ...]` to begin counting.  \n",
    "\n",
    "In English, this could be easy because we can intuitively use spaces to seperate every words.  \n",
    "> \"Of course we're celebrating the New Year!\"  \n",
    "> -> [\"Of\", \"course\", <u>\"we're\"</u>, \"celebrating\", \"the\", \"New\", <u>\"Year!\"</u>]\n",
    "\n",
    "However, some tokenizer may also treat punctuations and abbreviations as independant units:  \n",
    "> \"Of course we're celebrating the New Year!\"  \n",
    "> -> [\"Of\", \"course\", <span style=\"color: red\"><u>\"we\", \"'\", \"re\"</u></span>, \"celebrating\", \"the\", \"New\", <span style=\"color: red\"><u>\"Year\", \"!\"</u></span>]\n",
    "\n",
    "Also, you might want to treat some special terms as single tokens:  \n",
    "> \"Of course we're celebrating the New Year!\"  \n",
    "> -> [\"Of\", \"course\", \"we're\", \"celebrating\", \"the\", <span style=\"color: red\"><u>\"New Year!\"</u></span>]\n",
    "\n",
    "And in Chinese this becomes even trickier without the hint from spaces:  \n",
    "> \"今天天氣真好。\"  \n",
    "> -> [\"今天\", \"天氣\", \"真\", \"好\", \"。\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing what tokenization is, now we can start to build our own counter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**open the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm good. They're good. Hi!We are good.123!Yo.Hi.,sc,,,,Hello.\n",
      "The Project Gutenberg EBook\n"
     ]
    }
   ],
   "source": [
    "#file_path = os.path.join('.','big.txt') # change to where you put your data\n",
    "file_path = \"./big.txt\"\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red\">[ TODO ]</span> Implement your tokenization function here!**  \n",
    "\n",
    "In this assignment, we will use the following rules: \n",
    " 1. Ignore case (e.g., \"The\" is the same as \"the\")\n",
    " 2. Split by white spaces and punctuations\n",
    " 3. Ignore all punctuation\n",
    "\n",
    "Example:\n",
    "`\"Hello! I'm your TA!\" -> [\"hello\", \"i\", \"m\", \"your\", \"ta\"]`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(\"[\\w]+\", text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(\"[\\w+'\\w]+\", text)\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for i in tokens:\n",
    "        if i in punc:\n",
    "            tokens = tokens.replace(i, \"\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test your function!**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-1536110eca7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-d14fbdf64b43>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(text)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\">Expected output:</span> `['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency counting\n",
    "We have splitted the sentence into tokens! Now we can try to calculate the frequency.  \n",
    "<span style=\"color: red\">[ TODO ]</span> Try to <u>write a counter to sum up how many times the word shows up</u>.   \n",
    "<small>(*Hint: dict, defaultdict, counter, etc.)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency(tokens):\n",
    "    frequency = {}\n",
    "    for item in tokens: \n",
    "        if (item in frequency): \n",
    "            frequency[item] += 1\n",
    "        else: \n",
    "            frequency[item] = 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = calculate_frequency(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are too many entries in this counter, it may not be a good idea if we print them all.  \n",
    "<span style=\"color: red\">[ TODO ]</span> Let's <u>write an utility function to print only the words with top-N frequency</u>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 7685),\n",
       " ('m', 170),\n",
       " ('good', 747),\n",
       " ('they', 3939),\n",
       " ('re', 190),\n",
       " ('hi', 4),\n",
       " ('we', 1907),\n",
       " ('are', 3631),\n",
       " ('123', 11),\n",
       " ('yo', 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(counter.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad example\n",
    "'''\n",
    "def print_top_n(counter, n ):\n",
    "    sorted_dict = dict(sorted(counter.items(), key=lambda x: x[1],reverse = True))\n",
    "    cnt = 0 \n",
    "    required_cnt= n\n",
    "    for key, value in sorted_dict.items():\n",
    "        cnt += 1\n",
    "        if cnt > required_cnt:\n",
    "            break\n",
    "        print(\"{} {}\".format(key, value))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good example\n",
    "def print_top_n(counter: dict, n=10):\n",
    "    sorted_dict = dict(sorted(counter.items(), key=lambda x: x[1],reverse = True))\n",
    "    for key, value in list(sorted_dict.items())[:n]:\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 79809\n",
      "of 40024\n",
      "and 38312\n",
      "to 28765\n",
      "in 22023\n",
      "a 21124\n",
      "that 12512\n",
      "he 12401\n",
      "was 11410\n",
      "it 10681\n"
     ]
    }
   ],
   "source": [
    "print_top_n(counter, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\">Expected output:</span>\n",
    "```\n",
    "the 79809\n",
    "of 40024\n",
    "and 38312\n",
    "to 28765\n",
    "in 22023\n",
    "a 21124\n",
    "that 12512\n",
    "he 12401\n",
    "was 11410\n",
    "it 10681\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram\n",
    "\n",
    "Now we have known how to calculate the word frequency.  \n",
    "However, sometimes we not only want to know about single words, but also about the *phrases*, and here N-gram can be brought in.  \n",
    "\n",
    "**N-gram is a contiguous sequence of n items from a given sample of text or speech.** <small>(Definition from [wikipedia](https://en.wikipedia.org/wiki/N-gram))</small>  \n",
    "\n",
    "Consider the token list from previous exmaple: `[\"Of\", \"course\", \"we're\", \"celebrating\", \"the\", \"New\", \"Year!\"]` .  \n",
    "The 2-gram, or bigram, of this example is `[\"Of course\", \"course we're\", \"we're celebrating\", ...]` .  \n",
    "You may notice that the phrase \"of course\" now is bundled and counted together, and this is where N-gram has its power.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red\">[ TODO ]</span> In the following section, let's (1) <u>write a function to generate n-gram</u> and then (2) <u>calculate the frequency of grams</u> with different width.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(tokens, n=2):\n",
    "    ... # [ TODO ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(tokens, n=2):\n",
    "    grams = [tokens[i:i+n] for i in range(len(tokens)-n+1)]\n",
    "    tokens=[]\n",
    "    for i in grams:\n",
    "        str1 = \" \".join(i)\n",
    "        tokens.append(str1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i m', 'm good', 'good they', 'they re', 're good']\n"
     ]
    }
   ],
   "source": [
    "bigram = get_ngram(tokens, n=2)\n",
    "print(bigram[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\">Expected output:</span> `['the project', 'project gutenberg', 'gutenberg ebook', 'ebook of', 'of the']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the 2-gram is generated, we can use the same counter we built above to count the frequency.  \n",
    "<small>Note that if your counter couldn't work as expected here, you need to fix it above.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the 12528\n",
      "in the 6446\n",
      "to the 4464\n",
      "and the 3202\n",
      "on the 2526\n"
     ]
    }
   ],
   "source": [
    "bigram_counter = calculate_frequency(bigram)\n",
    "print_top_n(bigram_counter, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\">Expected output:</span>\n",
    "```\n",
    "of the 12528\n",
    "in the 6446\n",
    "to the 4464\n",
    "and the 3202\n",
    "on the 2526\n",
    "at the 2103\n",
    "by the 1937\n",
    "from the 1865\n",
    "with the 1735\n",
    "of a 1710\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congraturation! You've succefully built a simple ngram frequency calculator! ;)  \n",
    "Feel free to try building a trigram, 4-gram, etc. counter and observe the difference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA's Notes\n",
    "\n",
    "Assignment#1 is just a warm-up practice, so you don't need to hand it in this week.  \n",
    "However, **starting from the next week you'll need explain your implementation to TAs** after you finish your assignment.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with TA before you miss the deadline**</u> .  \n",
    "Note that **late submission will not be allowed**.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e56d558c252a4887cb9bc110bf182e2aa9946109639baab21a0ce4014d1d01c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
